{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee506abc",
   "metadata": {},
   "source": [
    "# M5 Walmart Sales Forecasting - Model Training\n",
    "\n",
    "This notebook implements and trains multiple time series forecasting models on the M5 dataset.\n",
    "\n",
    "## Models Implemented\n",
    "\n",
    "1. **SARIMA (Seasonal ARIMA)**: Statistical time series model with seasonal components\n",
    "2. **LSTM (Long Short-Term Memory)**: Deep learning model for sequence prediction\n",
    "3. **Prophet**: Facebook's time series forecasting tool\n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "- Use time-based train/validation split\n",
    "- Focus on highest selling product for detailed modeling\n",
    "- Implement proper evaluation metrics\n",
    "- Save trained models for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model imports\n",
    "from src.models.sarima_model import SarimaModel\n",
    "from src.models.lstm_model import LSTMModel\n",
    "from src.models.prophet_model import ProphetModel\n",
    "\n",
    "# Utility imports\n",
    "from src.data.data_loader import M5DataLoader\n",
    "from src.data.preprocessing import M5DataPreprocessor\n",
    "from src.visualization.plots import M5Visualizer\n",
    "from src.utils.config import get_config\n",
    "from src.utils.logger import setup_logger\n",
    "from src.utils.metrics import calculate_metrics\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = setup_logger('model_training')\n",
    "config = get_config()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Configuration loaded: {type(config)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74c9c3",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "processed_data_path = config.get('data.processed_data_path', 'data/processed/')\n",
    "\n",
    "print(\"Loading processed data...\")\n",
    "\n",
    "# Load full processed dataset\n",
    "try:\n",
    "    sales_processed = pd.read_parquet(f\"{processed_data_path}/sales_processed.parquet\")\n",
    "    print(f\"‚úÖ Loaded processed sales data: {sales_processed.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Processed data not found. Please run 02_feature_engineering.ipynb first.\")\n",
    "    raise\n",
    "\n",
    "# Load time series data for highest selling product\n",
    "try:\n",
    "    product_ts = pd.read_csv(f\"{processed_data_path}/highest_selling_product_ts.csv\", \n",
    "                            index_col=0, parse_dates=True)\n",
    "    print(f\"‚úÖ Loaded product time series: {product_ts.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Product time series not found. Using fallback method...\")\n",
    "    # Fallback: prepare data manually\n",
    "    preprocessor = M5DataPreprocessor()\n",
    "    highest_selling_product = preprocessor.get_highest_selling_product(sales_processed)\n",
    "    product_ts = preprocessor.prepare_time_series_data(sales_processed, highest_selling_product)\n",
    "    print(f\"‚úÖ Prepared product time series: {product_ts.shape}\")\n",
    "\n",
    "# Load feature metadata\n",
    "import json\n",
    "try:\n",
    "    with open(f\"{processed_data_path}/feature_metadata.json\", 'r') as f:\n",
    "        feature_metadata = json.load(f)\n",
    "    highest_selling_product = feature_metadata['highest_selling_product']\n",
    "    print(f\"‚úÖ Loaded metadata. Product: {highest_selling_product}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Feature metadata not found. Using default highest selling product.\")\n",
    "    preprocessor = M5DataPreprocessor()\n",
    "    highest_selling_product = preprocessor.get_highest_selling_product(sales_processed)\n",
    "\n",
    "print(f\"\\nTraining on product: {highest_selling_product}\")\n",
    "print(f\"Time series date range: {product_ts.index.min()} to {product_ts.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30260a0a",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time series data\n",
    "print(\"Preparing data for modeling...\")\n",
    "\n",
    "# Remove any rows with missing sales values\n",
    "product_ts_clean = product_ts.dropna(subset=['sales'])\n",
    "print(f\"Clean time series shape: {product_ts_clean.shape}\")\n",
    "\n",
    "# Define train/validation/test split dates\n",
    "train_end_date = '2016-03-27'  # Roughly 80% of data for training\n",
    "val_end_date = '2016-04-24'    # Next 28 days for validation\n",
    "# Test period: remaining dates\n",
    "\n",
    "print(f\"Data split strategy:\")\n",
    "print(f\"  Training period: {product_ts_clean.index.min()} to {train_end_date}\")\n",
    "print(f\"  Validation period: {train_end_date} to {val_end_date}\")\n",
    "print(f\"  Test period: {val_end_date} to {product_ts_clean.index.max()}\")\n",
    "\n",
    "# Create splits\n",
    "train_data = product_ts_clean[product_ts_clean.index <= train_end_date]\n",
    "val_data = product_ts_clean[(product_ts_clean.index > train_end_date) & \n",
    "                          (product_ts_clean.index <= val_end_date)]\n",
    "test_data = product_ts_clean[product_ts_clean.index > val_end_date]\n",
    "\n",
    "print(f\"\\nData split sizes:\")\n",
    "print(f\"  Training: {len(train_data)} days\")\n",
    "print(f\"  Validation: {len(val_data)} days\")\n",
    "print(f\"  Test: {len(test_data)} days\")\n",
    "\n",
    "# Visualize the splits\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train_data.index, train_data['sales'], label='Training', alpha=0.8)\n",
    "plt.plot(val_data.index, val_data['sales'], label='Validation', alpha=0.8)\n",
    "plt.plot(test_data.index, test_data['sales'], label='Test', alpha=0.8)\n",
    "plt.axvline(x=pd.to_datetime(train_end_date), color='red', linestyle='--', alpha=0.7, label='Train/Val Split')\n",
    "plt.axvline(x=pd.to_datetime(val_end_date), color='orange', linestyle='--', alpha=0.7, label='Val/Test Split')\n",
    "plt.title(f'Time Series Data Splits - {highest_selling_product}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa9106",
   "metadata": {},
   "source": [
    "## 3. Model 1: SARIMA (Seasonal ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SARIMA model\n",
    "print(\"\\nüéØ TRAINING SARIMA MODEL\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "sarima_model = SarimaModel()\n",
    "\n",
    "# Prepare data for SARIMA (needs just the time series)\n",
    "y_train = train_data['sales'].values\n",
    "y_val = val_data['sales'].values\n",
    "y_test = test_data['sales'].values\n",
    "\n",
    "print(f\"Training SARIMA on {len(y_train)} observations...\")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    sarima_model.fit(y_train, seasonal_periods=7)  # Weekly seasonality\n",
    "    print(\"‚úÖ SARIMA model trained successfully!\")\n",
    "    \n",
    "    # Print model summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(sarima_model.get_model_summary())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SARIMA training failed: {e}\")\n",
    "    print(\"Trying with simpler parameters...\")\n",
    "    \n",
    "    # Fallback with simpler parameters\n",
    "    sarima_model = SarimaModel(order=(1,1,1), seasonal_order=(1,1,1,7))\n",
    "    sarima_model.fit(y_train)\n",
    "    print(\"‚úÖ SARIMA model trained with fallback parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with SARIMA\n",
    "print(\"Making SARIMA predictions...\")\n",
    "\n",
    "# Predict on validation set\n",
    "sarima_val_pred = sarima_model.predict(len(y_val))\n",
    "print(f\"Validation predictions shape: {sarima_val_pred.shape}\")\n",
    "\n",
    "# Predict on test set\n",
    "# For SARIMA, we need to retrain on train+val for test predictions\n",
    "y_train_val = np.concatenate([y_train, y_val])\n",
    "sarima_model_full = SarimaModel()\n",
    "sarima_model_full.fit(y_train_val, seasonal_periods=7)\n",
    "sarima_test_pred = sarima_model_full.predict(len(y_test))\n",
    "print(f\"Test predictions shape: {sarima_test_pred.shape}\")\n",
    "\n",
    "# Calculate metrics\n",
    "sarima_val_metrics = calculate_metrics(y_val, sarima_val_pred)\n",
    "sarima_test_metrics = calculate_metrics(y_test, sarima_test_pred)\n",
    "\n",
    "print(\"\\nüìä SARIMA Performance:\")\n",
    "print(f\"Validation - RMSE: {sarima_val_metrics['rmse']:.3f}, MAE: {sarima_val_metrics['mae']:.3f}, MAPE: {sarima_val_metrics['mape']:.3f}%\")\n",
    "print(f\"Test - RMSE: {sarima_test_metrics['rmse']:.3f}, MAE: {sarima_test_metrics['mae']:.3f}, MAPE: {sarima_test_metrics['mape']:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SARIMA results\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(train_data.index, train_data['sales'], label='Training Data', alpha=0.7)\n",
    "\n",
    "# Plot validation data and predictions\n",
    "plt.plot(val_data.index, val_data['sales'], label='Validation Actual', color='green')\n",
    "plt.plot(val_data.index, sarima_val_pred, label='SARIMA Validation Pred', color='red', linestyle='--')\n",
    "\n",
    "# Plot test data and predictions\n",
    "plt.plot(test_data.index, test_data['sales'], label='Test Actual', color='blue')\n",
    "plt.plot(test_data.index, sarima_test_pred, label='SARIMA Test Pred', color='orange', linestyle='--')\n",
    "\n",
    "plt.axvline(x=pd.to_datetime(train_end_date), color='red', linestyle=':', alpha=0.7)\n",
    "plt.axvline(x=pd.to_datetime(val_end_date), color='orange', linestyle=':', alpha=0.7)\n",
    "\n",
    "plt.title('SARIMA Model Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8bf5d",
   "metadata": {},
   "source": [
    "## 4. Model 2: LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM model\n",
    "print(\"\\nü§ñ TRAINING LSTM MODEL\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Prepare features for LSTM\n",
    "feature_columns = ['sales']\n",
    "if 'lag_1' in train_data.columns:\n",
    "    feature_columns.extend(['lag_1', 'lag_7', 'rolling_mean_7'])\n",
    "if 'day_of_week' in train_data.columns:\n",
    "    feature_columns.extend(['day_of_week', 'month'])\n",
    "\n",
    "# Remove features with missing values\n",
    "available_features = [col for col in feature_columns if col in train_data.columns]\n",
    "print(f\"Using features for LSTM: {available_features}\")\n",
    "\n",
    "# Prepare data for LSTM\n",
    "X_train = train_data[available_features].fillna(0).values\n",
    "X_val = val_data[available_features].fillna(0).values\n",
    "X_test = test_data[available_features].fillna(0).values\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Initialize LSTM model\n",
    "lstm_model = LSTMModel(\n",
    "    sequence_length=28,  # Use 28 days of history\n",
    "    n_features=len(available_features),\n",
    "    lstm_units=50,\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "try:\n",
    "    # Train the model\n",
    "    lstm_model.fit(X_train, y_train)\n",
    "    print(\"‚úÖ LSTM model trained successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LSTM training failed: {e}\")\n",
    "    print(\"Note: LSTM requires TensorFlow/Keras. Install with: pip install tensorflow\")\n",
    "    lstm_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with LSTM (if model was trained successfully)\n",
    "if lstm_model is not None:\n",
    "    print(\"Making LSTM predictions...\")\n",
    "    \n",
    "    # Predict on validation set\n",
    "    lstm_val_pred = lstm_model.predict(X_val)\n",
    "    print(f\"Validation predictions shape: {lstm_val_pred.shape}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    # Retrain on train+val for test predictions\n",
    "    X_train_val = np.concatenate([X_train, X_val])\n",
    "    y_train_val = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    lstm_model_full = LSTMModel(\n",
    "        sequence_length=28,\n",
    "        n_features=len(available_features),\n",
    "        lstm_units=50,\n",
    "        epochs=30,  # Fewer epochs for retraining\n",
    "        batch_size=32\n",
    "    )\n",
    "    lstm_model_full.fit(X_train_val, y_train_val)\n",
    "    lstm_test_pred = lstm_model_full.predict(X_test)\n",
    "    print(f\"Test predictions shape: {lstm_test_pred.shape}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    lstm_val_metrics = calculate_metrics(y_val, lstm_val_pred)\n",
    "    lstm_test_metrics = calculate_metrics(y_test, lstm_test_pred)\n",
    "    \n",
    "    print(\"\\nüìä LSTM Performance:\")\n",
    "    print(f\"Validation - RMSE: {lstm_val_metrics['rmse']:.3f}, MAE: {lstm_val_metrics['mae']:.3f}, MAPE: {lstm_val_metrics['mape']:.3f}%\")\n",
    "    print(f\"Test - RMSE: {lstm_test_metrics['rmse']:.3f}, MAE: {lstm_test_metrics['mae']:.3f}, MAPE: {lstm_test_metrics['mape']:.3f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå LSTM model not available. Skipping predictions.\")\n",
    "    lstm_val_pred = np.zeros_like(y_val)\n",
    "    lstm_test_pred = np.zeros_like(y_test)\n",
    "    lstm_val_metrics = {'rmse': np.inf, 'mae': np.inf, 'mape': np.inf}\n",
    "    lstm_test_metrics = {'rmse': np.inf, 'mae': np.inf, 'mape': np.inf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM results (if available)\n",
    "if lstm_model is not None:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot training data\n",
    "    plt.plot(train_data.index, train_data['sales'], label='Training Data', alpha=0.7)\n",
    "    \n",
    "    # Plot validation data and predictions\n",
    "    plt.plot(val_data.index, val_data['sales'], label='Validation Actual', color='green')\n",
    "    plt.plot(val_data.index, lstm_val_pred, label='LSTM Validation Pred', color='red', linestyle='--')\n",
    "    \n",
    "    # Plot test data and predictions\n",
    "    plt.plot(test_data.index, test_data['sales'], label='Test Actual', color='blue')\n",
    "    plt.plot(test_data.index, lstm_test_pred, label='LSTM Test Pred', color='orange', linestyle='--')\n",
    "    \n",
    "    plt.axvline(x=pd.to_datetime(train_end_date), color='red', linestyle=':', alpha=0.7)\n",
    "    plt.axvline(x=pd.to_datetime(val_end_date), color='orange', linestyle=':', alpha=0.7)\n",
    "    \n",
    "    plt.title('LSTM Model Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"LSTM visualization skipped - model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2af83f",
   "metadata": {},
   "source": [
    "## 5. Model 3: Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b216b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Prophet model\n",
    "print(\"\\nüìà TRAINING PROPHET MODEL\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "# Prepare data for Prophet (needs 'ds' and 'y' columns)\n",
    "prophet_train = pd.DataFrame({\n",
    "    'ds': train_data.index,\n",
    "    'y': train_data['sales'].values\n",
    "})\n",
    "\n",
    "prophet_val = pd.DataFrame({\n",
    "    'ds': val_data.index,\n",
    "    'y': val_data['sales'].values\n",
    "})\n",
    "\n",
    "prophet_test = pd.DataFrame({\n",
    "    'ds': test_data.index,\n",
    "    'y': test_data['sales'].values\n",
    "})\n",
    "\n",
    "print(f\"Prophet training data shape: {prophet_train.shape}\")\n",
    "\n",
    "# Initialize Prophet model\n",
    "prophet_model = ProphetModel(\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True\n",
    ")\n",
    "\n",
    "print(\"Training Prophet model...\")\n",
    "try:\n",
    "    # Train the model\n",
    "    prophet_model.fit(prophet_train)\n",
    "    print(\"‚úÖ Prophet model trained successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Prophet training failed: {e}\")\n",
    "    print(\"Note: Prophet requires specific installation. Install with: pip install prophet\")\n",
    "    prophet_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with Prophet (if model was trained successfully)\n",
    "if prophet_model is not None:\n",
    "    print(\"Making Prophet predictions...\")\n",
    "    \n",
    "    # Predict on validation set\n",
    "    prophet_val_pred = prophet_model.predict(len(y_val))['yhat'].values\n",
    "    print(f\"Validation predictions shape: {prophet_val_pred.shape}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    # Retrain on train+val for test predictions\n",
    "    prophet_train_val = pd.DataFrame({\n",
    "        'ds': pd.concat([train_data, val_data]).index,\n",
    "        'y': np.concatenate([y_train, y_val])\n",
    "    })\n",
    "    \n",
    "    prophet_model_full = ProphetModel(\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True\n",
    "    )\n",
    "    prophet_model_full.fit(prophet_train_val)\n",
    "    prophet_test_pred = prophet_model_full.predict(len(y_test))['yhat'].values\n",
    "    print(f\"Test predictions shape: {prophet_test_pred.shape}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    prophet_val_metrics = calculate_metrics(y_val, prophet_val_pred)\n",
    "    prophet_test_metrics = calculate_metrics(y_test, prophet_test_pred)\n",
    "    \n",
    "    print(\"\\nüìä Prophet Performance:\")\n",
    "    print(f\"Validation - RMSE: {prophet_val_metrics['rmse']:.3f}, MAE: {prophet_val_metrics['mae']:.3f}, MAPE: {prophet_val_metrics['mape']:.3f}%\")\n",
    "    print(f\"Test - RMSE: {prophet_test_metrics['rmse']:.3f}, MAE: {prophet_test_metrics['mae']:.3f}, MAPE: {prophet_test_metrics['mape']:.3f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Prophet model not available. Skipping predictions.\")\n",
    "    prophet_val_pred = np.zeros_like(y_val)\n",
    "    prophet_test_pred = np.zeros_like(y_test)\n",
    "    prophet_val_metrics = {'rmse': np.inf, 'mae': np.inf, 'mape': np.inf}\n",
    "    prophet_test_metrics = {'rmse': np.inf, 'mae': np.inf, 'mape': np.inf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Prophet results (if available)\n",
    "if prophet_model is not None:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot training data\n",
    "    plt.plot(train_data.index, train_data['sales'], label='Training Data', alpha=0.7)\n",
    "    \n",
    "    # Plot validation data and predictions\n",
    "    plt.plot(val_data.index, val_data['sales'], label='Validation Actual', color='green')\n",
    "    plt.plot(val_data.index, prophet_val_pred, label='Prophet Validation Pred', color='red', linestyle='--')\n",
    "    \n",
    "    # Plot test data and predictions\n",
    "    plt.plot(test_data.index, test_data['sales'], label='Test Actual', color='blue')\n",
    "    plt.plot(test_data.index, prophet_test_pred, label='Prophet Test Pred', color='orange', linestyle='--')\n",
    "    \n",
    "    plt.axvline(x=pd.to_datetime(train_end_date), color='red', linestyle=':', alpha=0.7)\n",
    "    plt.axvline(x=pd.to_datetime(val_end_date), color='orange', linestyle=':', alpha=0.7)\n",
    "    \n",
    "    plt.title('Prophet Model Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Prophet visualization skipped - model not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1a3e7",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7334fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "print(\"\\nüèÜ MODEL COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create results dataframe\n",
    "results_data = {\n",
    "    'Model': ['SARIMA', 'LSTM', 'Prophet'],\n",
    "    'Validation_RMSE': [sarima_val_metrics['rmse'], lstm_val_metrics['rmse'], prophet_val_metrics['rmse']],\n",
    "    'Validation_MAE': [sarima_val_metrics['mae'], lstm_val_metrics['mae'], prophet_val_metrics['mae']],\n",
    "    'Validation_MAPE': [sarima_val_metrics['mape'], lstm_val_metrics['mape'], prophet_val_metrics['mape']],\n",
    "    'Test_RMSE': [sarima_test_metrics['rmse'], lstm_test_metrics['rmse'], prophet_test_metrics['rmse']],\n",
    "    'Test_MAE': [sarima_test_metrics['mae'], lstm_test_metrics['mae'], prophet_test_metrics['mae']],\n",
    "    'Test_MAPE': [sarima_test_metrics['mape'], lstm_test_metrics['mape'], prophet_test_metrics['mape']]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(results_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['Test_RMSE'].idxmin()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nü•á Best performing model: {best_model_name}\")\n",
    "print(f\"   Test RMSE: {results_df.loc[best_model_idx, 'Test_RMSE']:.3f}\")\n",
    "print(f\"   Test MAE: {results_df.loc[best_model_idx, 'Test_MAE']:.3f}\")\n",
    "print(f\"   Test MAPE: {results_df.loc[best_model_idx, 'Test_MAPE']:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Validation RMSE\n",
    "axes[0,0].bar(results_df['Model'], results_df['Validation_RMSE'])\n",
    "axes[0,0].set_title('Validation RMSE')\n",
    "axes[0,0].set_ylabel('RMSE')\n",
    "\n",
    "# Test RMSE\n",
    "axes[0,1].bar(results_df['Model'], results_df['Test_RMSE'])\n",
    "axes[0,1].set_title('Test RMSE')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "\n",
    "# Validation MAPE\n",
    "axes[1,0].bar(results_df['Model'], results_df['Validation_MAPE'])\n",
    "axes[1,0].set_title('Validation MAPE')\n",
    "axes[1,0].set_ylabel('MAPE (%)')\n",
    "\n",
    "# Test MAPE\n",
    "axes[1,1].bar(results_df['Model'], results_df['Test_MAPE'])\n",
    "axes[1,1].set_title('Test MAPE')\n",
    "axes[1,1].set_ylabel('MAPE (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all predictions on test set\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(test_data.index, test_data['sales'], label='Actual', color='black', linewidth=2)\n",
    "\n",
    "# Plot predictions from all models\n",
    "plt.plot(test_data.index, sarima_test_pred, label='SARIMA', alpha=0.8, linestyle='--')\n",
    "if lstm_model is not None:\n",
    "    plt.plot(test_data.index, lstm_test_pred, label='LSTM', alpha=0.8, linestyle='--')\n",
    "if prophet_model is not None:\n",
    "    plt.plot(test_data.index, prophet_test_pred, label='Prophet', alpha=0.8, linestyle='--')\n",
    "\n",
    "plt.title('Model Predictions Comparison - Test Set')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb368952",
   "metadata": {},
   "source": [
    "## 7. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05910e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = config.get('models.model_path', 'models/')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving trained models...\")\n",
    "\n",
    "# Save SARIMA model\n",
    "try:\n",
    "    sarima_model.save_model(f\"{models_dir}/sarima_model.pkl\")\n",
    "    print(f\"‚úÖ SARIMA model saved to {models_dir}/sarima_model.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save SARIMA model: {e}\")\n",
    "\n",
    "# Save LSTM model\n",
    "if lstm_model is not None:\n",
    "    try:\n",
    "        lstm_model.save_model(f\"{models_dir}/lstm_model.h5\")\n",
    "        print(f\"‚úÖ LSTM model saved to {models_dir}/lstm_model.h5\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save LSTM model: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  LSTM model not available for saving\")\n",
    "\n",
    "# Save Prophet model\n",
    "if prophet_model is not None:\n",
    "    try:\n",
    "        prophet_model.save_model(f\"{models_dir}/prophet_model.pkl\")\n",
    "        print(f\"‚úÖ Prophet model saved to {models_dir}/prophet_model.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save Prophet model: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Prophet model not available for saving\")\n",
    "\n",
    "# Save results summary\n",
    "results_df.to_csv(f\"{models_dir}/model_comparison_results.csv\", index=False)\n",
    "print(f\"‚úÖ Results summary saved to {models_dir}/model_comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1654b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed training metadata\n",
    "training_metadata = {\n",
    "    'product_id': highest_selling_product,\n",
    "    'training_period': {\n",
    "        'start': str(train_data.index.min()),\n",
    "        'end': str(train_data.index.max()),\n",
    "        'days': len(train_data)\n",
    "    },\n",
    "    'validation_period': {\n",
    "        'start': str(val_data.index.min()),\n",
    "        'end': str(val_data.index.max()),\n",
    "        'days': len(val_data)\n",
    "    },\n",
    "    'test_period': {\n",
    "        'start': str(test_data.index.min()),\n",
    "        'end': str(test_data.index.max()),\n",
    "        'days': len(test_data)\n",
    "    },\n",
    "    'model_performance': results_df.to_dict('records'),\n",
    "    'best_model': best_model_name,\n",
    "    'features_used': available_features,\n",
    "    'data_splits': {\n",
    "        'train_end_date': train_end_date,\n",
    "        'val_end_date': val_end_date\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{models_dir}/training_metadata.json\", 'w') as f:\n",
    "    json.dump(training_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Training metadata saved to {models_dir}/training_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72104c72",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb28daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nüìä TRAINED ON PRODUCT: {highest_selling_product}\")\n",
    "print(f\"   Training period: {len(train_data)} days\")\n",
    "print(f\"   Validation period: {len(val_data)} days\")\n",
    "print(f\"   Test period: {len(test_data)} days\")\n",
    "\n",
    "print(\"\\nü§ñ MODELS TRAINED:\")\n",
    "models_trained = []\n",
    "if sarima_model:\n",
    "    models_trained.append(\"‚úÖ SARIMA\")\n",
    "if lstm_model:\n",
    "    models_trained.append(\"‚úÖ LSTM\")\n",
    "else:\n",
    "    models_trained.append(\"‚ùå LSTM (TensorFlow required)\")\n",
    "if prophet_model:\n",
    "    models_trained.append(\"‚úÖ Prophet\")\n",
    "else:\n",
    "    models_trained.append(\"‚ùå Prophet (Prophet package required)\")\n",
    "\n",
    "for model in models_trained:\n",
    "    print(f\"   {model}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test RMSE: {results_df.loc[best_model_idx, 'Test_RMSE']:.3f}\")\n",
    "print(f\"   Test MAPE: {results_df.loc[best_model_idx, 'Test_MAPE']:.3f}%\")\n",
    "\n",
    "print(\"\\nüíæ FILES SAVED:\")\n",
    "print(f\"   ‚Ä¢ {models_dir}/sarima_model.pkl\")\n",
    "if lstm_model:\n",
    "    print(f\"   ‚Ä¢ {models_dir}/lstm_model.h5\")\n",
    "if prophet_model:\n",
    "    print(f\"   ‚Ä¢ {models_dir}/prophet_model.pkl\")\n",
    "print(f\"   ‚Ä¢ {models_dir}/model_comparison_results.csv\")\n",
    "print(f\"   ‚Ä¢ {models_dir}/training_metadata.json\")\n",
    "\n",
    "print(\"\\nüîß TO INSTALL MISSING DEPENDENCIES:\")\n",
    "if lstm_model is None:\n",
    "    print(\"   pip install tensorflow\")\n",
    "if prophet_model is None:\n",
    "    print(\"   pip install prophet\")\n",
    "\n",
    "print(\"\\n‚û°Ô∏è  Next step: Run 04_model_evaluation.ipynb for detailed analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
