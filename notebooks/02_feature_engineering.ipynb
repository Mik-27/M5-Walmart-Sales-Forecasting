{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8967d71e",
   "metadata": {},
   "source": [
    "# M5 Walmart Sales Forecasting - Feature Engineering\n",
    "\n",
    "This notebook performs comprehensive feature engineering on the M5 dataset to prepare data for time series modeling.\n",
    "\n",
    "## Feature Engineering Strategy\n",
    "\n",
    "1. **Lag Features**: Previous sales values\n",
    "2. **Rolling Statistics**: Moving averages and standard deviations\n",
    "3. **Temporal Features**: Date-based features\n",
    "4. **Price Features**: Price changes and trends\n",
    "5. **Event Features**: Holiday and event indicators\n",
    "6. **Hierarchical Features**: Category and store aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.data_loader import M5DataLoader\n",
    "from src.data.preprocessing import M5DataPreprocessor\n",
    "from src.visualization.plots import M5Visualizer\n",
    "from src.utils.config import get_config\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = setup_logger('feature_engineering')\n",
    "config = get_config()\n",
    "\n",
    "# Initialize preprocessor and visualizer\n",
    "preprocessor = M5DataPreprocessor()\n",
    "visualizer = M5Visualizer()\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6020f",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = config.get('data.raw_data_path', 'data/raw/')\n",
    "loader = M5DataLoader(data_path)\n",
    "\n",
    "calendar, sales, prices = loader.load_all_data()\n",
    "print(\"Data loaded successfully!\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Sales data shape: {sales.shape}\")\n",
    "print(f\"Calendar data shape: {calendar.shape}\")\n",
    "print(f\"Prices data shape: {prices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape sales data to long format\n",
    "print(\"Reshaping sales data to long format...\")\n",
    "sales_long = preprocessor.reshape_sales_data(sales, calendar)\n",
    "\n",
    "print(f\"Reshaped data shape: {sales_long.shape}\")\n",
    "print(\"\\nSample of reshaped data:\")\n",
    "print(sales_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15983d2e",
   "metadata": {},
   "source": [
    "## 2. Create Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lag configuration\n",
    "lag_features = config.get('features.lag_features', [1, 2, 3, 7, 14, 28])\n",
    "print(f\"Creating lag features for lags: {lag_features}\")\n",
    "\n",
    "# Create lag features\n",
    "sales_with_lags = preprocessor.create_lag_features(\n",
    "    sales_long, \n",
    "    target_col='sales', \n",
    "    lags=lag_features\n",
    ")\n",
    "\n",
    "print(f\"Data shape after adding lag features: {sales_with_lags.shape}\")\n",
    "print(\"\\nLag features created:\")\n",
    "lag_cols = [col for col in sales_with_lags.columns if col.startswith('lag_')]\n",
    "print(lag_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00378a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lag features for a sample item\n",
    "sample_item = sales_with_lags['id'].iloc[0]\n",
    "sample_data = sales_with_lags[sales_with_lags['id'] == sample_item].head(10)\n",
    "\n",
    "print(f\"Lag features for sample item {sample_item}:\")\n",
    "display_cols = ['date', 'sales'] + lag_cols\n",
    "print(sample_data[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502207b9",
   "metadata": {},
   "source": [
    "## 3. Create Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rolling window configuration\n",
    "rolling_windows = config.get('features.rolling_windows', [7, 14, 28])\n",
    "print(f\"Creating rolling features for windows: {rolling_windows}\")\n",
    "\n",
    "# Create rolling features\n",
    "sales_with_rolling = preprocessor.create_rolling_features(\n",
    "    sales_with_lags,\n",
    "    target_col='sales',\n",
    "    windows=rolling_windows\n",
    ")\n",
    "\n",
    "print(f\"Data shape after adding rolling features: {sales_with_rolling.shape}\")\n",
    "print(\"\\nRolling features created:\")\n",
    "rolling_cols = [col for col in sales_with_rolling.columns if col.startswith('rolling_')]\n",
    "print(rolling_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rolling features for a sample item\n",
    "sample_data_rolling = sales_with_rolling[sales_with_rolling['id'] == sample_item].iloc[50:150]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(sample_data_rolling['date'], sample_data_rolling['sales'], label='Actual Sales', alpha=0.7)\n",
    "plt.plot(sample_data_rolling['date'], sample_data_rolling['rolling_mean_7'], label='7-day MA')\n",
    "plt.plot(sample_data_rolling['date'], sample_data_rolling['rolling_mean_14'], label='14-day MA')\n",
    "plt.plot(sample_data_rolling['date'], sample_data_rolling['rolling_mean_28'], label='28-day MA')\n",
    "\n",
    "plt.title(f'Rolling Averages for {sample_item}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f827ebf",
   "metadata": {},
   "source": [
    "## 4. Create Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features\n",
    "print(\"Creating temporal features...\")\n",
    "sales_with_temporal = preprocessor.create_temporal_features(sales_with_rolling)\n",
    "\n",
    "print(f\"Data shape after adding temporal features: {sales_with_temporal.shape}\")\n",
    "print(\"\\nTemporal features created:\")\n",
    "temporal_cols = ['day_of_week', 'day_of_month', 'day_of_year', 'week_of_year', \n",
    "                'month', 'quarter', 'year', 'is_weekend', 'is_month_start', 'is_month_end']\n",
    "print([col for col in temporal_cols if col in sales_with_temporal.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081de5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns\n",
    "print(\"Temporal Pattern Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Sales by day of week\n",
    "dow_sales = sales_with_temporal.groupby('day_of_week')['sales'].mean()\n",
    "print(\"\\nAverage sales by day of week:\")\n",
    "dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "for dow, sales_avg in dow_sales.items():\n",
    "    print(f\"{dow_names[dow]}: {sales_avg:.2f}\")\n",
    "\n",
    "# Sales by month\n",
    "month_sales = sales_with_temporal.groupby('month')['sales'].mean()\n",
    "print(\"\\nAverage sales by month:\")\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "for month, sales_avg in month_sales.items():\n",
    "    print(f\"{month_names[month-1]}: {sales_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4aadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Day of week pattern\n",
    "dow_sales.plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Average Sales by Day of Week')\n",
    "axes[0,0].set_xlabel('Day of Week (0=Monday)')\n",
    "axes[0,0].set_ylabel('Average Sales')\n",
    "\n",
    "# Month pattern\n",
    "month_sales.plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('Average Sales by Month')\n",
    "axes[0,1].set_xlabel('Month')\n",
    "axes[0,1].set_ylabel('Average Sales')\n",
    "\n",
    "# Weekend vs weekday\n",
    "weekend_sales = sales_with_temporal.groupby('is_weekend')['sales'].mean()\n",
    "weekend_sales.plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average Sales: Weekday vs Weekend')\n",
    "axes[1,0].set_xlabel('Is Weekend (0=No, 1=Yes)')\n",
    "axes[1,0].set_ylabel('Average Sales')\n",
    "\n",
    "# Quarter pattern\n",
    "quarter_sales = sales_with_temporal.groupby('quarter')['sales'].mean()\n",
    "quarter_sales.plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Average Sales by Quarter')\n",
    "axes[1,1].set_xlabel('Quarter')\n",
    "axes[1,1].set_ylabel('Average Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f92d9c",
   "metadata": {},
   "source": [
    "## 5. Create Price Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price features\n",
    "print(\"Creating price features...\")\n",
    "sales_with_prices = preprocessor.create_price_features(sales_with_temporal, prices)\n",
    "\n",
    "print(f\"Data shape after adding price features: {sales_with_prices.shape}\")\n",
    "print(\"\\nPrice features created:\")\n",
    "price_cols = [col for col in sales_with_prices.columns if 'price' in col.lower()]\n",
    "print(price_cols)\n",
    "\n",
    "# Check price coverage\n",
    "price_coverage = sales_with_prices['sell_price'].notna().mean()\n",
    "print(f\"\\nPrice coverage: {price_coverage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price-sales relationship\n",
    "print(\"Price-Sales Relationship Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Correlation between price and sales\n",
    "price_sales_corr = sales_with_prices[['sales', 'sell_price']].corr().iloc[0,1]\n",
    "print(f\"Correlation between price and sales: {price_sales_corr:.3f}\")\n",
    "\n",
    "# Price elasticity analysis\n",
    "price_change_sales_corr = sales_with_prices[['sales', 'price_change_pct']].corr().iloc[0,1]\n",
    "print(f\"Correlation between sales and price change %: {price_change_sales_corr:.3f}\")\n",
    "\n",
    "# Visualize price-sales relationship\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sample_price_data = sales_with_prices[sales_with_prices['sell_price'].notna()].sample(10000)\n",
    "plt.scatter(sample_price_data['sell_price'], sample_price_data['sales'], alpha=0.1)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Price vs Sales Relationship')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "price_change_data = sales_with_prices[sales_with_prices['price_change_pct'].notna()].sample(10000)\n",
    "plt.scatter(price_change_data['price_change_pct'], price_change_data['sales'], alpha=0.1)\n",
    "plt.xlabel('Price Change %')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Price Change vs Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85923a",
   "metadata": {},
   "source": [
    "## 6. Create Event Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create event features\n",
    "print(\"Creating event features...\")\n",
    "sales_with_events = preprocessor.create_event_features(sales_with_prices)\n",
    "\n",
    "print(f\"Data shape after adding event features: {sales_with_events.shape}\")\n",
    "print(\"\\nEvent features created:\")\n",
    "event_cols = [col for col in sales_with_events.columns if 'event' in col.lower() or 'snap' in col.lower()]\n",
    "print(event_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze event impact on sales\n",
    "print(\"Event Impact Analysis:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Sales with and without events\n",
    "event_sales = sales_with_events.groupby('has_any_event')['sales'].mean()\n",
    "print(\"Average sales:\")\n",
    "print(f\"  Without events: {event_sales[0]:.2f}\")\n",
    "print(f\"  With events: {event_sales[1]:.2f}\")\n",
    "print(f\"  Event uplift: {(event_sales[1]/event_sales[0] - 1)*100:.1f}%\")\n",
    "\n",
    "# SNAP impact\n",
    "snap_sales = sales_with_events.groupby('snap_any')['sales'].mean()\n",
    "print(\"\\nSNAP benefits impact:\")\n",
    "print(f\"  Without SNAP: {snap_sales[0]:.2f}\")\n",
    "print(f\"  With SNAP: {snap_sales[1]:.2f}\")\n",
    "print(f\"  SNAP uplift: {(snap_sales[1]/snap_sales[0] - 1)*100:.1f}%\")\n",
    "\n",
    "# Event type impact\n",
    "print(\"\\nEvent type impact:\")\n",
    "event_type_cols = [col for col in sales_with_events.columns if col.startswith('event_type_')]\n",
    "for col in event_type_cols:\n",
    "    event_type_sales = sales_with_events.groupby(col)['sales'].mean()\n",
    "    if len(event_type_sales) > 1:\n",
    "        uplift = (event_type_sales[1]/event_type_sales[0] - 1)*100\n",
    "        print(f\"  {col.replace('event_type_', '').title()}: {uplift:.1f}% uplift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df675b0",
   "metadata": {},
   "source": [
    "## 7. Handle Missing Values and Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in engineered features\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "missing_values = sales_with_events.isnull().sum()\n",
    "missing_pct = (missing_values / len(sales_with_events)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f385e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in lag and rolling features\n",
    "print(\"Handling missing values...\")\n",
    "\n",
    "# For lag features, forward fill within each item group\n",
    "lag_cols = [col for col in sales_with_events.columns if col.startswith('lag_')]\n",
    "rolling_cols = [col for col in sales_with_events.columns if col.startswith('rolling_')]\n",
    "\n",
    "for col in lag_cols + rolling_cols:\n",
    "    sales_with_events[col] = sales_with_events.groupby('id')[col].fillna(method='bfill')\n",
    "    sales_with_events[col] = sales_with_events.groupby('id')[col].fillna(0)\n",
    "\n",
    "# For price features, use forward fill and backward fill\n",
    "price_feature_cols = ['sell_price', 'price_change', 'price_change_pct', 'price_momentum_7']\n",
    "for col in price_feature_cols:\n",
    "    if col in sales_with_events.columns:\n",
    "        sales_with_events[col] = sales_with_events.groupby(['store_id', 'item_id'])[col].fillna(method='ffill')\n",
    "        sales_with_events[col] = sales_with_events.groupby(['store_id', 'item_id'])[col].fillna(method='bfill')\n",
    "        sales_with_events[col] = sales_with_events[col].fillna(sales_with_events[col].median())\n",
    "\n",
    "print(\"Missing values handled.\")\n",
    "\n",
    "# Check remaining missing values\n",
    "remaining_missing = sales_with_events.isnull().sum().sum()\n",
    "print(f\"Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d408d7",
   "metadata": {},
   "source": [
    "## 8. Feature Selection and Engineering Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ca49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize engineered features\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Final dataset shape: {sales_with_events.shape}\")\n",
    "print(f\"Original sales dataset shape: {sales.shape}\")\n",
    "print(f\"Features added: {sales_with_events.shape[1] - len(sales.columns)}\")\n",
    "\n",
    "# Categorize features\n",
    "feature_categories = {\n",
    "    'Original': ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales'],\n",
    "    'Temporal': [col for col in sales_with_events.columns if col in temporal_cols],\n",
    "    'Lag': [col for col in sales_with_events.columns if col.startswith('lag_')],\n",
    "    'Rolling': [col for col in sales_with_events.columns if col.startswith('rolling_')],\n",
    "    'Price': [col for col in sales_with_events.columns if 'price' in col.lower()],\n",
    "    'Event': [col for col in sales_with_events.columns if 'event' in col.lower() or 'snap' in col.lower()],\n",
    "    'Calendar': [col for col in sales_with_events.columns if col in calendar.columns and col not in ['d', 'date']]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature categories:\")\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"  {category}: {len(features)} features\")\n",
    "    if len(features) <= 10:\n",
    "        print(f\"    {features}\")\n",
    "    else:\n",
    "        print(f\"    {features[:5]} ... {features[-2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (correlation with target)\n",
    "print(\"\\nFeature Correlation with Sales Target:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Select numeric features for correlation analysis\n",
    "numeric_features = sales_with_events.select_dtypes(include=[np.number]).columns\n",
    "feature_correlations = sales_with_events[numeric_features].corr()['sales'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 features by correlation with sales:\")\n",
    "print(feature_correlations.head(16).iloc[1:])  # Exclude sales itself\n",
    "\n",
    "# Visualize feature correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_correlations.head(16).iloc[1:]\n",
    "plt.barh(range(len(top_features)), top_features.values)\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Absolute Correlation with Sales')\n",
    "plt.title('Top 15 Features by Correlation with Sales')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51657b",
   "metadata": {},
   "source": [
    "## 9. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71eff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "modeling_features = [\n",
    "    # Identifier features\n",
    "    'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'date',\n",
    "    # Target\n",
    "    'sales',\n",
    "    # Lag features\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_7', 'lag_14', 'lag_28',\n",
    "    # Rolling features\n",
    "    'rolling_mean_7', 'rolling_mean_14', 'rolling_mean_28',\n",
    "    'rolling_std_7', 'rolling_std_14', 'rolling_std_28',\n",
    "    # Temporal features\n",
    "    'day_of_week', 'day_of_month', 'month', 'quarter', 'is_weekend',\n",
    "    # Price features (if available)\n",
    "    'sell_price', 'price_change_pct',\n",
    "    # Event features\n",
    "    'has_any_event', 'snap_any',\n",
    "    # Calendar features\n",
    "    'wm_yr_wk', 'wday'\n",
    "]\n",
    "\n",
    "# Filter features that actually exist in the dataset\n",
    "available_features = [feat for feat in modeling_features if feat in sales_with_events.columns]\n",
    "modeling_data = sales_with_events[available_features].copy()\n",
    "\n",
    "print(f\"Selected {len(available_features)} features for modeling:\")\n",
    "print(available_features)\n",
    "print(f\"\\nModeling dataset shape: {modeling_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest selling product for focused modeling\n",
    "highest_selling_product = preprocessor.get_highest_selling_product(sales)\n",
    "print(f\"Highest selling product: {highest_selling_product}\")\n",
    "\n",
    "# Prepare time series data for this product\n",
    "product_time_series = preprocessor.prepare_time_series_data(modeling_data, highest_selling_product)\n",
    "print(f\"Time series data shape: {product_time_series.shape}\")\n",
    "print(f\"Date range: {product_time_series.index.min()} to {product_time_series.index.max()}\")\n",
    "\n",
    "# Display sample of time series data\n",
    "print(\"\\nSample time series data:\")\n",
    "print(product_time_series.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331869e",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "processed_data_path = config.get('data.processed_data_path', 'data/processed/')\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Save full processed dataset\n",
    "modeling_data.to_parquet(f\"{processed_data_path}/sales_processed.parquet\", index=False)\n",
    "print(f\"Full processed dataset saved to {processed_data_path}/sales_processed.parquet\")\n",
    "\n",
    "# Save time series data for highest selling product\n",
    "product_time_series.to_csv(f\"{processed_data_path}/highest_selling_product_ts.csv\")\n",
    "print(f\"Product time series saved to {processed_data_path}/highest_selling_product_ts.csv\")\n",
    "\n",
    "# Save feature engineering metadata\n",
    "feature_metadata = {\n",
    "    'highest_selling_product': highest_selling_product,\n",
    "    'feature_categories': feature_categories,\n",
    "    'modeling_features': available_features,\n",
    "    'dataset_shape': modeling_data.shape,\n",
    "    'feature_correlations': feature_correlations.head(20).to_dict(),\n",
    "    'processing_config': {\n",
    "        'lag_features': lag_features,\n",
    "        'rolling_windows': rolling_windows,\n",
    "        'temporal_features': temporal_cols\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{processed_data_path}/feature_metadata.json\", 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Feature metadata saved to {processed_data_path}/feature_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9451e",
   "metadata": {},
   "source": [
    "## 11. Feature Engineering Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63821fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n✅ FEATURES CREATED:\")\n",
    "print(f\"   • {len(lag_cols)} lag features (1, 2, 3, 7, 14, 28 days)\")\n",
    "print(f\"   • {len(rolling_cols)} rolling statistics (7, 14, 28 day windows)\")\n",
    "print(f\"   • {len([col for col in sales_with_events.columns if col in temporal_cols])} temporal features\")\n",
    "print(f\"   • {len([col for col in sales_with_events.columns if 'price' in col.lower()])} price features\")\n",
    "print(f\"   • {len([col for col in sales_with_events.columns if 'event' in col.lower() or 'snap' in col.lower()])} event features\")\n",
    "\n",
    "print(\"\\n📊 DATA INSIGHTS:\")\n",
    "print(f\"   • Event uplift: {(event_sales[1]/event_sales[0] - 1)*100:.1f}%\")\n",
    "print(f\"   • SNAP benefits uplift: {(snap_sales[1]/snap_sales[0] - 1)*100:.1f}%\")\n",
    "print(f\"   • Weekend vs weekday sales: {(weekend_sales[1]/weekend_sales[0] - 1)*100:.1f}%\")\n",
    "print(f\"   • Price-sales correlation: {price_sales_corr:.3f}\")\n",
    "\n",
    "print(\"\\n💾 FILES SAVED:\")\n",
    "print(f\"   • {processed_data_path}/sales_processed.parquet\")\n",
    "print(f\"   • {processed_data_path}/highest_selling_product_ts.csv\")\n",
    "print(f\"   • {processed_data_path}/feature_metadata.json\")\n",
    "\n",
    "print(\"\\n🎯 READY FOR MODELING:\")\n",
    "print(f\"   • Dataset shape: {modeling_data.shape}\")\n",
    "print(f\"   • Features selected: {len(available_features)}\")\n",
    "print(f\"   • Highest selling product: {highest_selling_product}\")\n",
    "print(f\"   • No missing values in key features\")\n",
    "\n",
    "print(\"\\n➡️  Next step: Run 03_model_training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
