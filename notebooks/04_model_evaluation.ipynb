{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4f5c49",
   "metadata": {},
   "source": [
    "# M5 Walmart Sales Forecasting - Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation and analysis of the trained forecasting models.\n",
    "\n",
    "## Evaluation Components\n",
    "\n",
    "1. **Model Performance Analysis**: Detailed metrics and comparisons\n",
    "2. **Residual Analysis**: Understanding prediction errors\n",
    "3. **Feature Importance**: What drives the predictions\n",
    "4. **Seasonal Decomposition**: Understanding model behavior\n",
    "5. **Business Impact**: Translation to business metrics\n",
    "6. **Model Diagnostics**: Statistical validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Model imports\n",
    "from src.models.sarima_model import SarimaModel\n",
    "from src.models.lstm_model import LSTMModel\n",
    "from src.models.prophet_model import ProphetModel\n",
    "\n",
    "# Utility imports\n",
    "from src.visualization.plots import M5Visualizer\n",
    "from src.utils.config import get_config\n",
    "from src.utils.logger import setup_logger\n",
    "from src.utils.metrics import calculate_metrics\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = setup_logger('model_evaluation')\n",
    "config = get_config()\n",
    "visualizer = M5Visualizer()\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953829c",
   "metadata": {},
   "source": [
    "## 1. Load Training Results and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results and metadata\n",
    "models_dir = config.get('models.model_path', 'models/')\n",
    "processed_data_path = config.get('data.processed_data_path', 'data/processed/')\n",
    "\n",
    "print(\"Loading training results and metadata...\")\n",
    "\n",
    "# Load model comparison results\n",
    "try:\n",
    "    results_df = pd.read_csv(f\"{models_dir}/model_comparison_results.csv\")\n",
    "    print(f\"‚úÖ Loaded model comparison results: {results_df.shape}\")\n",
    "    print(results_df)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Model comparison results not found. Please run 03_model_training.ipynb first.\")\n",
    "    raise\n",
    "\n",
    "# Load training metadata\n",
    "try:\n",
    "    with open(f\"{models_dir}/training_metadata.json\", 'r') as f:\n",
    "        training_metadata = json.load(f)\n",
    "    print(f\"‚úÖ Loaded training metadata\")\n",
    "    \n",
    "    highest_selling_product = training_metadata['product_id']\n",
    "    best_model = training_metadata['best_model']\n",
    "    print(f\"   Product: {highest_selling_product}\")\n",
    "    print(f\"   Best model: {best_model}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Training metadata not found. Using defaults.\")\n",
    "    highest_selling_product = \"HOBBIES_1_001_CA_1_validation\"\n",
    "    best_model = \"SARIMA\"\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    sales_processed = pd.read_parquet(f\"{processed_data_path}/sales_processed.parquet\")\n",
    "    product_ts = pd.read_csv(f\"{processed_data_path}/highest_selling_product_ts.csv\", \n",
    "                            index_col=0, parse_dates=True)\n",
    "    print(f\"‚úÖ Loaded processed data: {sales_processed.shape}, {product_ts.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Processed data not found. Please run feature engineering first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3557de",
   "metadata": {},
   "source": [
    "## 2. Recreate Train/Test Splits and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the train/test splits used in training\n",
    "train_end_date = training_metadata['data_splits']['train_end_date']\n",
    "val_end_date = training_metadata['data_splits']['val_end_date']\n",
    "\n",
    "print(f\"Recreating data splits:\")\n",
    "print(f\"  Train end: {train_end_date}\")\n",
    "print(f\"  Validation end: {val_end_date}\")\n",
    "\n",
    "# Clean time series data\n",
    "product_ts_clean = product_ts.dropna(subset=['sales'])\n",
    "\n",
    "# Create splits\n",
    "train_data = product_ts_clean[product_ts_clean.index <= train_end_date]\n",
    "val_data = product_ts_clean[(product_ts_clean.index > train_end_date) & \n",
    "                          (product_ts_clean.index <= val_end_date)]\n",
    "test_data = product_ts_clean[product_ts_clean.index > val_end_date]\n",
    "\n",
    "print(f\"\\nData split sizes:\")\n",
    "print(f\"  Training: {len(train_data)} days\")\n",
    "print(f\"  Validation: {len(val_data)} days\")\n",
    "print(f\"  Test: {len(test_data)} days\")\n",
    "\n",
    "# Extract target values\n",
    "y_train = train_data['sales'].values\n",
    "y_val = val_data['sales'].values\n",
    "y_test = test_data['sales'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36360b0",
   "metadata": {},
   "source": [
    "## 3. Load Trained Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a31e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate SARIMA model\n",
    "print(\"Loading and evaluating SARIMA model...\")\n",
    "try:\n",
    "    sarima_model = SarimaModel()\n",
    "    sarima_model.load_model(f\"{models_dir}/sarima_model.pkl\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    sarima_val_pred = sarima_model.predict(len(y_val))\n",
    "    \n",
    "    # For test predictions, retrain on train+val\n",
    "    y_train_val = np.concatenate([y_train, y_val])\n",
    "    sarima_model_full = SarimaModel()\n",
    "    sarima_model_full.fit(y_train_val, seasonal_periods=7)\n",
    "    sarima_test_pred = sarima_model_full.predict(len(y_test))\n",
    "    \n",
    "    print(\"‚úÖ SARIMA predictions generated\")\n",
    "    sarima_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SARIMA model loading failed: {e}\")\n",
    "    sarima_val_pred = np.zeros_like(y_val)\n",
    "    sarima_test_pred = np.zeros_like(y_test)\n",
    "    sarima_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb847733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate LSTM model\n",
    "print(\"Loading and evaluating LSTM model...\")\n",
    "try:\n",
    "    lstm_model = LSTMModel(sequence_length=28, n_features=1)\n",
    "    lstm_model.load_model(f\"{models_dir}/lstm_model.h5\")\n",
    "    \n",
    "    # Prepare features for LSTM\n",
    "    feature_columns = ['sales']\n",
    "    X_train = train_data[feature_columns].fillna(0).values\n",
    "    X_val = val_data[feature_columns].fillna(0).values\n",
    "    X_test = test_data[feature_columns].fillna(0).values\n",
    "    \n",
    "    # Generate predictions\n",
    "    lstm_val_pred = lstm_model.predict(X_val)\n",
    "    lstm_test_pred = lstm_model.predict(X_test)\n",
    "    \n",
    "    print(\"‚úÖ LSTM predictions generated\")\n",
    "    lstm_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LSTM model loading failed: {e}\")\n",
    "    lstm_val_pred = np.zeros_like(y_val)\n",
    "    lstm_test_pred = np.zeros_like(y_test)\n",
    "    lstm_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate Prophet model\n",
    "print(\"Loading and evaluating Prophet model...\")\n",
    "try:\n",
    "    prophet_model = ProphetModel()\n",
    "    prophet_model.load_model(f\"{models_dir}/prophet_model.pkl\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    prophet_val_pred = prophet_model.predict(len(y_val))['yhat'].values\n",
    "    \n",
    "    # For test predictions, retrain on train+val\n",
    "    prophet_train_val = pd.DataFrame({\n",
    "        'ds': pd.concat([train_data, val_data]).index,\n",
    "        'y': np.concatenate([y_train, y_val])\n",
    "    })\n",
    "    prophet_model_full = ProphetModel()\n",
    "    prophet_model_full.fit(prophet_train_val)\n",
    "    prophet_test_pred = prophet_model_full.predict(len(y_test))['yhat'].values\n",
    "    \n",
    "    print(\"‚úÖ Prophet predictions generated\")\n",
    "    prophet_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Prophet model loading failed: {e}\")\n",
    "    prophet_val_pred = np.zeros_like(y_val)\n",
    "    prophet_test_pred = np.zeros_like(y_test)\n",
    "    prophet_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25504f96",
   "metadata": {},
   "source": [
    "## 4. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c94828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics for all models\n",
    "print(\"\\nüìä COMPREHENSIVE PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def extended_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate extended metrics for model evaluation\"\"\"\n",
    "    metrics = calculate_metrics(y_true, y_pred)\n",
    "    \n",
    "    # Additional metrics\n",
    "    metrics['mean_error'] = np.mean(y_pred - y_true)\n",
    "    metrics['std_error'] = np.std(y_pred - y_true)\n",
    "    metrics['max_error'] = np.max(np.abs(y_pred - y_true))\n",
    "    metrics['r2_score'] = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    \n",
    "    # Direction accuracy (for forecasting)\n",
    "    if len(y_true) > 1:\n",
    "        true_direction = np.diff(y_true) > 0\n",
    "        pred_direction = np.diff(y_pred) > 0\n",
    "        metrics['direction_accuracy'] = np.mean(true_direction == pred_direction) * 100\n",
    "    else:\n",
    "        metrics['direction_accuracy'] = np.nan\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for all models\n",
    "models_predictions = {\n",
    "    'SARIMA': {'val': sarima_val_pred, 'test': sarima_test_pred, 'available': sarima_available},\n",
    "    'LSTM': {'val': lstm_val_pred, 'test': lstm_test_pred, 'available': lstm_available},\n",
    "    'Prophet': {'val': prophet_val_pred, 'test': prophet_test_pred, 'available': prophet_available}\n",
    "}\n",
    "\n",
    "detailed_results = []\n",
    "\n",
    "for model_name, preds in models_predictions.items():\n",
    "    if preds['available']:\n",
    "        val_metrics = extended_metrics(y_val, preds['val'], model_name)\n",
    "        test_metrics = extended_metrics(y_test, preds['test'], model_name)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': 'Validation',\n",
    "            **val_metrics\n",
    "        })\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': 'Test',\n",
    "            **test_metrics\n",
    "        })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "print(detailed_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd572b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detailed performance metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "metrics_to_plot = ['rmse', 'mae', 'mape', 'r2_score', 'direction_accuracy', 'max_error']\n",
    "metric_titles = ['RMSE', 'MAE', 'MAPE (%)', 'R¬≤ Score', 'Direction Accuracy (%)', 'Max Error']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, metric_titles)):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Filter data for this metric\n",
    "    test_data_metric = detailed_df[detailed_df['Dataset'] == 'Test']\n",
    "    \n",
    "    if metric in test_data_metric.columns:\n",
    "        axes[row, col].bar(test_data_metric['Model'], test_data_metric[metric])\n",
    "        axes[row, col].set_title(f'Test {title}')\n",
    "        axes[row, col].set_ylabel(title)\n",
    "        \n",
    "        # Rotate x-axis labels if needed\n",
    "        if len(test_data_metric) > 3:\n",
    "            axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec57c7",
   "metadata": {},
   "source": [
    "## 5. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e61665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for each model\n",
    "print(\"\\nüîç RESIDUAL ANALYSIS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "def analyze_residuals(y_true, y_pred, model_name):\n",
    "    \"\"\"Analyze prediction residuals\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    print(f\"\\n{model_name} Residuals:\")\n",
    "    print(f\"  Mean: {np.mean(residuals):.3f}\")\n",
    "    print(f\"  Std: {np.std(residuals):.3f}\")\n",
    "    print(f\"  Skewness: {stats.skew(residuals):.3f}\")\n",
    "    print(f\"  Kurtosis: {stats.kurtosis(residuals):.3f}\")\n",
    "    \n",
    "    # Normality test\n",
    "    _, p_value = stats.normaltest(residuals)\n",
    "    print(f\"  Normality test p-value: {p_value:.3f}\")\n",
    "    print(f\"  Residuals normal: {'Yes' if p_value > 0.05 else 'No'}\")\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "# Analyze residuals for available models\n",
    "residuals_dict = {}\n",
    "\n",
    "for model_name, preds in models_predictions.items():\n",
    "    if preds['available']:\n",
    "        residuals_dict[model_name] = analyze_residuals(y_test, preds['test'], model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residuals\n",
    "available_models = [name for name, preds in models_predictions.items() if preds['available']]\n",
    "n_models = len(available_models)\n",
    "\n",
    "if n_models > 0:\n",
    "    fig, axes = plt.subplots(n_models, 3, figsize=(15, 5 * n_models))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, model_name in enumerate(available_models):\n",
    "        residuals = residuals_dict[model_name]\n",
    "        \n",
    "        # Residuals over time\n",
    "        axes[idx, 0].plot(test_data.index, residuals)\n",
    "        axes[idx, 0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[idx, 0].set_title(f'{model_name} - Residuals Over Time')\n",
    "        axes[idx, 0].set_xlabel('Date')\n",
    "        axes[idx, 0].set_ylabel('Residuals')\n",
    "        \n",
    "        # Residuals histogram\n",
    "        axes[idx, 1].hist(residuals, bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[idx, 1].axvline(x=np.mean(residuals), color='red', linestyle='--', label='Mean')\n",
    "        axes[idx, 1].set_title(f'{model_name} - Residuals Distribution')\n",
    "        axes[idx, 1].set_xlabel('Residuals')\n",
    "        axes[idx, 1].set_ylabel('Frequency')\n",
    "        axes[idx, 1].legend()\n",
    "        \n",
    "        # Q-Q plot\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=axes[idx, 2])\n",
    "        axes[idx, 2].set_title(f'{model_name} - Q-Q Plot')\n",
    "        axes[idx, 2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models available for residual analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe51e7",
   "metadata": {},
   "source": [
    "## 6. Error Analysis by Time and Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis by different dimensions\n",
    "print(\"\\nüìà ERROR ANALYSIS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "def error_analysis_by_dimension(y_true, y_pred, dates, model_name):\n",
    "    \"\"\"Analyze errors by different dimensions\"\"\"\n",
    "    errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Error by day of week\n",
    "    df_temp = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'error': errors,\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    df_temp['day_of_week'] = df_temp['date'].dt.day_name()\n",
    "    df_temp['week'] = df_temp['date'].dt.isocalendar().week\n",
    "    df_temp['month'] = df_temp['date'].dt.month\n",
    "    \n",
    "    print(f\"\\n{model_name} Error Analysis:\")\n",
    "    print(\"Error by Day of Week:\")\n",
    "    dow_errors = df_temp.groupby('day_of_week')['error'].mean().sort_values(ascending=False)\n",
    "    print(dow_errors.round(3))\n",
    "    \n",
    "    print(\"\\nError by Sales Magnitude:\")\n",
    "    df_temp['sales_quartile'] = pd.qcut(df_temp['actual'], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
    "    quartile_errors = df_temp.groupby('sales_quartile')['error'].mean()\n",
    "    print(quartile_errors.round(3))\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Perform error analysis for available models\n",
    "error_analysis_results = {}\n",
    "\n",
    "for model_name, preds in models_predictions.items():\n",
    "    if preds['available']:\n",
    "        error_analysis_results[model_name] = error_analysis_by_dimension(\n",
    "            y_test, preds['test'], test_data.index, model_name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error patterns\n",
    "if error_analysis_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Error by day of week\n",
    "    for model_name, df_temp in error_analysis_results.items():\n",
    "        dow_errors = df_temp.groupby('day_of_week')['error'].mean()\n",
    "        axes[0, 0].plot(dow_errors.index, dow_errors.values, marker='o', label=model_name)\n",
    "    \n",
    "    axes[0, 0].set_title('Average Error by Day of Week')\n",
    "    axes[0, 0].set_xlabel('Day of Week')\n",
    "    axes[0, 0].set_ylabel('Mean Absolute Error')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Error by sales magnitude\n",
    "    for model_name, df_temp in error_analysis_results.items():\n",
    "        quartile_errors = df_temp.groupby('sales_quartile')['error'].mean()\n",
    "        axes[0, 1].plot(quartile_errors.index, quartile_errors.values, marker='o', label=model_name)\n",
    "    \n",
    "    axes[0, 1].set_title('Average Error by Sales Quartile')\n",
    "    axes[0, 1].set_xlabel('Sales Quartile')\n",
    "    axes[0, 1].set_ylabel('Mean Absolute Error')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Error over time\n",
    "    for model_name, df_temp in error_analysis_results.items():\n",
    "        axes[1, 0].plot(df_temp['date'], df_temp['error'], alpha=0.7, label=model_name)\n",
    "    \n",
    "    axes[1, 0].set_title('Error Over Time')\n",
    "    axes[1, 0].set_xlabel('Date')\n",
    "    axes[1, 0].set_ylabel('Absolute Error')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Actual vs Predicted scatter\n",
    "    for model_name, df_temp in error_analysis_results.items():\n",
    "        axes[1, 1].scatter(df_temp['actual'], df_temp['predicted'], alpha=0.6, label=model_name)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), min([preds['test'].min() for preds in models_predictions.values() if preds['available']]))\n",
    "    max_val = max(y_test.max(), max([preds['test'].max() for preds in models_predictions.values() if preds['available']]))\n",
    "    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Perfect Prediction')\n",
    "    \n",
    "    axes[1, 1].set_title('Actual vs Predicted')\n",
    "    axes[1, 1].set_xlabel('Actual Sales')\n",
    "    axes[1, 1].set_ylabel('Predicted Sales')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24f807",
   "metadata": {},
   "source": [
    "## 7. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact analysis\n",
    "print(\"\\nüíº BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def business_impact_analysis(y_true, y_pred, model_name):\n",
    "    \"\"\"Analyze business impact of predictions\"\"\"\n",
    "    \n",
    "    # Assume average product price and profit margins\n",
    "    avg_price = 10.0  # Average price per unit\n",
    "    profit_margin = 0.2  # 20% profit margin\n",
    "    \n",
    "    # Calculate revenue impact\n",
    "    actual_revenue = np.sum(y_true) * avg_price\n",
    "    predicted_revenue = np.sum(y_pred) * avg_price\n",
    "    revenue_error = predicted_revenue - actual_revenue\n",
    "    revenue_error_pct = (revenue_error / actual_revenue) * 100\n",
    "    \n",
    "    # Calculate inventory impact (overstocking/understocking)\n",
    "    inventory_error = np.sum(np.abs(y_pred - y_true))\n",
    "    inventory_cost = inventory_error * avg_price * 0.1  # 10% carrying cost\n",
    "    \n",
    "    # Lost sales (understocking)\n",
    "    understocking = np.sum(np.maximum(0, y_true - y_pred))\n",
    "    lost_sales_cost = understocking * avg_price * profit_margin\n",
    "    \n",
    "    # Overstocking cost\n",
    "    overstocking = np.sum(np.maximum(0, y_pred - y_true))\n",
    "    overstocking_cost = overstocking * avg_price * 0.05  # 5% markdown cost\n",
    "    \n",
    "    print(f\"\\n{model_name} Business Impact:\")\n",
    "    print(f\"  Actual revenue: ${actual_revenue:,.2f}\")\n",
    "    print(f\"  Predicted revenue: ${predicted_revenue:,.2f}\")\n",
    "    print(f\"  Revenue error: ${revenue_error:,.2f} ({revenue_error_pct:.2f}%)\")\n",
    "    print(f\"  Inventory carrying cost: ${inventory_cost:,.2f}\")\n",
    "    print(f\"  Lost sales cost: ${lost_sales_cost:,.2f}\")\n",
    "    print(f\"  Overstocking cost: ${overstocking_cost:,.2f}\")\n",
    "    print(f\"  Total operational cost: ${inventory_cost + lost_sales_cost + overstocking_cost:,.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'actual_revenue': actual_revenue,\n",
    "        'predicted_revenue': predicted_revenue,\n",
    "        'revenue_error': revenue_error,\n",
    "        'revenue_error_pct': revenue_error_pct,\n",
    "        'inventory_cost': inventory_cost,\n",
    "        'lost_sales_cost': lost_sales_cost,\n",
    "        'overstocking_cost': overstocking_cost,\n",
    "        'total_cost': inventory_cost + lost_sales_cost + overstocking_cost\n",
    "    }\n",
    "\n",
    "# Calculate business impact for available models\n",
    "business_impacts = {}\n",
    "\n",
    "for model_name, preds in models_predictions.items():\n",
    "    if preds['available']:\n",
    "        business_impacts[model_name] = business_impact_analysis(y_test, preds['test'], model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6466faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize business impact\n",
    "if business_impacts:\n",
    "    business_df = pd.DataFrame(business_impacts).T\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Revenue error\n",
    "    axes[0].bar(business_df.index, business_df['revenue_error_pct'])\n",
    "    axes[0].set_title('Revenue Prediction Error (%)')\n",
    "    axes[0].set_ylabel('Error Percentage')\n",
    "    axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Total operational cost\n",
    "    axes[1].bar(business_df.index, business_df['total_cost'])\n",
    "    axes[1].set_title('Total Operational Cost ($)')\n",
    "    axes[1].set_ylabel('Cost ($)')\n",
    "    \n",
    "    # Cost breakdown\n",
    "    cost_components = ['inventory_cost', 'lost_sales_cost', 'overstocking_cost']\n",
    "    bottom = np.zeros(len(business_df))\n",
    "    \n",
    "    for component in cost_components:\n",
    "        axes[2].bar(business_df.index, business_df[component], bottom=bottom, \n",
    "                   label=component.replace('_', ' ').title())\n",
    "        bottom += business_df[component]\n",
    "    \n",
    "    axes[2].set_title('Cost Breakdown')\n",
    "    axes[2].set_ylabel('Cost ($)')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print business impact summary\n",
    "    print(\"\\nüí∞ BUSINESS IMPACT SUMMARY:\")\n",
    "    best_business_model = business_df['total_cost'].idxmin()\n",
    "    print(f\"Most cost-effective model: {best_business_model}\")\n",
    "    print(f\"Lowest operational cost: ${business_df.loc[best_business_model, 'total_cost']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab64938",
   "metadata": {},
   "source": [
    "## 8. Model Diagnostics and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical validation tests\n",
    "print(\"\\nüî¨ MODEL DIAGNOSTICS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "def statistical_validation(y_true, y_pred, model_name):\n",
    "    \"\"\"Perform statistical validation tests\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    print(f\"\\n{model_name} Statistical Tests:\")\n",
    "    \n",
    "    # Ljung-Box test for residual autocorrelation\n",
    "    try:\n",
    "        from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "        lb_stat, lb_pvalue = acorr_ljungbox(residuals, lags=10, return_df=False)\n",
    "        print(f\"  Ljung-Box test p-value: {lb_pvalue[-1]:.3f}\")\n",
    "        print(f\"  Residuals independent: {'Yes' if lb_pvalue[-1] > 0.05 else 'No'}\")\n",
    "    except ImportError:\n",
    "        print(\"  Ljung-Box test: Not available (requires statsmodels)\")\n",
    "    \n",
    "    # Jarque-Bera test for normality\n",
    "    jb_stat, jb_pvalue = stats.jarque_bera(residuals)\n",
    "    print(f\"  Jarque-Bera test p-value: {jb_pvalue:.3f}\")\n",
    "    print(f\"  Residuals normal: {'Yes' if jb_pvalue > 0.05 else 'No'}\")\n",
    "    \n",
    "    # Durbin-Watson test for autocorrelation\n",
    "    try:\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        dw_stat = durbin_watson(residuals)\n",
    "        print(f\"  Durbin-Watson statistic: {dw_stat:.3f}\")\n",
    "        print(f\"  Autocorrelation: {'Low' if 1.5 < dw_stat < 2.5 else 'High'}\")\n",
    "    except ImportError:\n",
    "        print(\"  Durbin-Watson test: Not available (requires statsmodels)\")\n",
    "    \n",
    "    # Heteroscedasticity test (Breusch-Pagan)\n",
    "    try:\n",
    "        from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "        bp_stat, bp_pvalue, _, _ = het_breuschpagan(residuals, np.column_stack([y_pred]))\n",
    "        print(f\"  Breusch-Pagan test p-value: {bp_pvalue:.3f}\")\n",
    "        print(f\"  Homoscedastic: {'Yes' if bp_pvalue > 0.05 else 'No'}\")\n",
    "    except (ImportError, Exception):\n",
    "        print(\"  Breusch-Pagan test: Not available\")\n",
    "\n",
    "# Perform statistical validation for available models\n",
    "for model_name, preds in models_predictions.items():\n",
    "    if preds['available']:\n",
    "        statistical_validation(y_test, preds['test'], model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f7c36",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis (for applicable models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fe1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\nüéØ FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# For models that support feature importance\n",
    "if 'lstm_model' in locals() and lstm_model is not None:\n",
    "    print(\"\\nLSTM Feature Analysis:\")\n",
    "    print(\"  LSTM uses sequential patterns in the data\")\n",
    "    print(\"  Most important: Recent sales values (last 28 days)\")\n",
    "    print(\"  Secondary: Seasonal patterns and trends\")\n",
    "\n",
    "if 'sarima_model' in locals() and sarima_model is not None:\n",
    "    print(\"\\nSARIMA Feature Analysis:\")\n",
    "    print(\"  SARIMA automatically identifies:\")\n",
    "    print(\"  - Autoregressive components (recent values)\")\n",
    "    print(\"  - Seasonal components (weekly patterns)\")\n",
    "    print(\"  - Moving average components (error correction)\")\n",
    "    \n",
    "    # Try to get SARIMA coefficients if available\n",
    "    try:\n",
    "        summary = sarima_model.get_model_summary()\n",
    "        print(\"  \\nModel coefficients available in summary\")\n",
    "    except:\n",
    "        print(\"  Model summary not available\")\n",
    "\n",
    "if 'prophet_model' in locals() and prophet_model is not None:\n",
    "    print(\"\\nProphet Feature Analysis:\")\n",
    "    print(\"  Prophet decomposes sales into:\")\n",
    "    print(\"  - Trend component (long-term direction)\")\n",
    "    print(\"  - Seasonal components (weekly, monthly, yearly)\")\n",
    "    print(\"  - Holiday effects (if specified)\")\n",
    "    print(\"  - Residual noise\")\n",
    "\n",
    "# Correlation analysis with features\n",
    "print(\"\\nüìä FEATURE CORRELATION ANALYSIS:\")\n",
    "feature_corr = product_ts_clean.corr()['sales'].abs().sort_values(ascending=False)\n",
    "print(\"Top features correlated with sales:\")\n",
    "print(feature_corr.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d640bb",
   "metadata": {},
   "source": [
    "## 10. Evaluation Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation summary\n",
    "print(\"\\nüìã COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Find best performing model across different metrics\n",
    "if len(detailed_df) > 0:\n",
    "    test_results = detailed_df[detailed_df['Dataset'] == 'Test']\n",
    "    \n",
    "    best_rmse = test_results.loc[test_results['rmse'].idxmin(), 'Model']\n",
    "    best_mae = test_results.loc[test_results['mae'].idxmin(), 'Model']\n",
    "    best_mape = test_results.loc[test_results['mape'].idxmin(), 'Model']\n",
    "    best_r2 = test_results.loc[test_results['r2_score'].idxmax(), 'Model']\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST MODELS BY METRIC:\")\n",
    "    print(f\"  Best RMSE: {best_rmse}\")\n",
    "    print(f\"  Best MAE: {best_mae}\")\n",
    "    print(f\"  Best MAPE: {best_mape}\")\n",
    "    print(f\"  Best R¬≤: {best_r2}\")\n",
    "    \n",
    "    if business_impacts:\n",
    "        best_business = min(business_impacts.keys(), key=lambda x: business_impacts[x]['total_cost'])\n",
    "        print(f\"  Best Business Impact: {best_business}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"\\n1. MODEL SELECTION:\")\n",
    "if sarima_available:\n",
    "    print(\"   ‚úÖ SARIMA: Good for stationary time series with clear seasonal patterns\")\n",
    "    print(\"      - Pros: Interpretable, fast, works well with limited data\")\n",
    "    print(\"      - Cons: Assumes stationarity, limited feature integration\")\n",
    "\n",
    "if lstm_available:\n",
    "    print(\"   ‚úÖ LSTM: Best for complex patterns and multivariate features\")\n",
    "    print(\"      - Pros: Handles non-linear patterns, uses multiple features\")\n",
    "    print(\"      - Cons: Requires more data, less interpretable, longer training\")\n",
    "\n",
    "if prophet_available:\n",
    "    print(\"   ‚úÖ Prophet: Excellent for business forecasting with holidays\")\n",
    "    print(\"      - Pros: Handles holidays, robust to missing data, interpretable components\")\n",
    "    print(\"      - Cons: May overfit with limited data, less flexible\")\n",
    "\n",
    "print(\"\\n2. IMPLEMENTATION STRATEGY:\")\n",
    "print(\"   üìà Start with SARIMA for baseline forecasts\")\n",
    "print(\"   ü§ñ Use LSTM when you have rich feature sets\")\n",
    "print(\"   üìä Apply Prophet for business planning and holiday impact\")\n",
    "print(\"   üîÑ Consider ensemble methods combining multiple models\")\n",
    "\n",
    "print(\"\\n3. OPERATIONAL CONSIDERATIONS:\")\n",
    "print(\"   ‚è±Ô∏è  Model retraining frequency: Weekly for SARIMA, Monthly for LSTM/Prophet\")\n",
    "print(\"   üíæ Data requirements: Minimum 2 years for seasonal patterns\")\n",
    "print(\"   üîç Monitor model drift using validation metrics\")\n",
    "print(\"   üìä Implement A/B testing for model selection\")\n",
    "\n",
    "print(\"\\n4. BUSINESS IMPACT:\")\n",
    "if business_impacts:\n",
    "    total_costs = [impact['total_cost'] for impact in business_impacts.values()]\n",
    "    avg_cost = np.mean(total_costs)\n",
    "    print(f\"   üí∞ Average operational cost: ${avg_cost:,.2f}\")\n",
    "    print(f\"   üìâ Focus on reducing overstocking and understocking costs\")\n",
    "    print(f\"   üéØ Target direction accuracy > 70% for trend following\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   üîÑ Scale to multiple products using the best performing model\")\n",
    "print(\"   üìä Implement automated model monitoring and alerting\")\n",
    "print(\"   üß™ Experiment with ensemble methods and advanced features\")\n",
    "print(\"   üìà Integrate with inventory management systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fbfddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "print(\"\\nüíæ SAVING EVALUATION RESULTS...\")\n",
    "\n",
    "evaluation_dir = \"output/evaluation/\"\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Save detailed results\n",
    "if len(detailed_df) > 0:\n",
    "    detailed_df.to_csv(f\"{evaluation_dir}/detailed_model_evaluation.csv\", index=False)\n",
    "    print(f\"‚úÖ Detailed evaluation saved to {evaluation_dir}/detailed_model_evaluation.csv\")\n",
    "\n",
    "# Save business impact analysis\n",
    "if business_impacts:\n",
    "    business_impact_df = pd.DataFrame(business_impacts).T\n",
    "    business_impact_df.to_csv(f\"{evaluation_dir}/business_impact_analysis.csv\")\n",
    "    print(f\"‚úÖ Business impact analysis saved to {evaluation_dir}/business_impact_analysis.csv\")\n",
    "\n",
    "# Save evaluation summary\n",
    "evaluation_summary = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'product_evaluated': highest_selling_product,\n",
    "    'test_period': {\n",
    "        'start': str(test_data.index.min()),\n",
    "        'end': str(test_data.index.max()),\n",
    "        'days': len(test_data)\n",
    "    },\n",
    "    'models_evaluated': list(models_predictions.keys()),\n",
    "    'available_models': [name for name, preds in models_predictions.items() if preds['available']],\n",
    "    'best_models': {\n",
    "        'rmse': best_rmse if 'best_rmse' in locals() else None,\n",
    "        'mae': best_mae if 'best_mae' in locals() else None,\n",
    "        'mape': best_mape if 'best_mape' in locals() else None,\n",
    "        'r2': best_r2 if 'best_r2' in locals() else None,\n",
    "        'business_impact': best_business if 'best_business' in locals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{evaluation_dir}/evaluation_summary.json\", 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Evaluation summary saved to {evaluation_dir}/evaluation_summary.json\")\n",
    "\n",
    "print(\"\\nüéâ MODEL EVALUATION COMPLETE!\")\n",
    "print(f\"üìÅ All results saved in: {evaluation_dir}\")\n",
    "print(\"üìä Review the detailed analysis above for model selection guidance\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
